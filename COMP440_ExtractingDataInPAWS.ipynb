{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mweiher15/CollectiveIntelCapstone/blob/main/COMP440_ExtractingDataInPAWS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the code below in PAWS notebook to extract the Wikipedia page assessments. The code was provided by Morten Warncke-Wang and Isaac Johnson."
      ],
      "metadata": {
        "id": "MF2QxqGHX-vh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F9oD36aVeH0"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import pymysql\n",
        "import requests\n",
        "from scipy.stats import spearmanr\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "days_worth_of_data = 28 "
      ],
      "metadata": {
        "id": "1VxlafK2Vuzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Groundtruth data is held in MariaDB replica databases accessible via PAWS\n",
        "\n",
        "def make_connection(wikidb):\n",
        "    \"\"\"Connects to a host and database of the same name.\"\"\"\n",
        "    return pymysql.connect(\n",
        "        host=f\"{wikidb}.analytics.db.svc.wikimedia.cloud\",\n",
        "        read_default_file=\"~/.my.cnf\",\n",
        "        database=f\"{wikidb}_p\",\n",
        "        charset='utf8'\n",
        "    )\n",
        "\n",
        "def query(conn, query):\n",
        "    \"\"\"Execute a SQL query against the connection, and return **all** the results.\"\"\"\n",
        "    with conn.cursor() as cur:\n",
        "        cur.execute(query)\n",
        "        data = cur.fetchall()\n",
        "        return data\n",
        "    \n",
        "def get_assessments(wikidb):\n",
        "    conn = make_connection(wikidb)\n",
        "    results = query(conn, groundtruth_query)\n",
        "    conn.close()\n",
        "    return results\n",
        "\n",
        "def assessments_to_df(results, wikidb):\n",
        "    df = pd.DataFrame(results, columns=['page_id', 'revid', 'quality'])\n",
        "    print(f'{len(df)} rows to start for {wikidb}.')\n",
        "    df = df.sort_values(by=['page_id', 'revid'])\n",
        "    df = df.drop_duplicates(subset='page_id', keep=\"last\")  # keep most recent assessment\n",
        "    print(f'{len(df)} rows after dropping duplicate pages.')\n",
        "    df['quality'] = df['quality'].apply(lambda x: x.decode('utf-8'))\n",
        "    if wikidb == 'frwiki':\n",
        "        df['enqual'] = df['quality'].apply(frqual_to_enqual)\n",
        "    elif wikidb == 'arwiki':\n",
        "        df['enqual'] = df['quality'].apply(arqual_to_enqual)\n",
        "    elif wikidb == 'trwiki':\n",
        "        df['enqual'] = df['quality'].apply(trqual_to_enqual)\n",
        "    elif wikidb == 'huwiki':\n",
        "        df['enqual'] = df['quality'].apply(huqual_to_enqual)\n",
        "    else:\n",
        "        df['enqual'] = df['quality'].apply(lambda x: x if x in enqual_categories else None)\n",
        "    df = df[~pd.isnull(df['enqual'])]\n",
        "    print(f'{len(df)} rows after removing unknown/missing ratings.')\n",
        "    df['qual_float'] = df['enqual'].apply(enqual_to_float)\n",
        "    print(\"\\nSample:\")\n",
        "    print(df.head(10))\n",
        "    print(\"\\nQual distribution:\")\n",
        "    print(df['enqual'].value_counts())\n",
        "    df['wiki_db'] = wikidb\n",
        "    return df\n",
        "\n",
        "# get groundtruth assessments from the previous X days\n",
        "four_weeks_ago = datetime.today() - timedelta(days=days_worth_of_data)\n",
        "start_ts = f'{four_weeks_ago.year}{str(four_weeks_ago.month).rjust(2, \"0\")}{str(four_weeks_ago.day).rjust(2, \"0\")}000000'\n",
        "\n",
        "groundtruth_query = f\"\"\"\n",
        "SELECT\n",
        "  pa_page_id AS page_id,\n",
        "  pa_page_revision AS rev_id,\n",
        "  pa_class AS qual_score\n",
        "  \n",
        "FROM page_assessments\n",
        "INNER JOIN revision\n",
        "  ON (pa_page_revision = rev_id)\n",
        "WHERE\n",
        "  rev_timestamp > {start_ts}\n",
        "\"\"\"\n",
        "\n",
        "#  pa_page_id in (305, 624, 765) AND\n",
        "print(groundtruth_query)"
      ],
      "metadata": {
        "id": "02mlwubIW3kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A-class are left out because there are so few\n",
        "# Descriptions: https://en.wikipedia.org/wiki/Wikipedia:Content_assessment\n",
        "enqual_categories = ['Stub', 'Start', 'C' , 'B', 'GA', 'FA']\n",
        "\n",
        "# functions for converting between quality classes and scores in different languages\n",
        "def float_to_enqual(qual_score):\n",
        "    # set based on:\n",
        "    # en_sample.groupby('enqual')['pred_qual_float'].median().reindex(enqual_categories)\n",
        "    # and equivalent for fr/ar with a little tweaking to create more separation\n",
        "    # and keep labels within correct group (start/stub; c/b; ga/fa)\n",
        "    if qual_score <= 0.36:\n",
        "        return 'Stub'\n",
        "    elif qual_score <= 0.54:\n",
        "        return 'Start'\n",
        "    elif qual_score <= 0.65:\n",
        "        return 'C'\n",
        "    elif qual_score <= 0.78:\n",
        "        return 'B'\n",
        "    elif qual_score <= 0.88:\n",
        "        return 'GA'\n",
        "    elif qual_score <= 1:\n",
        "        return 'FA'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def enqual_to_float(enqual):\n",
        "    # place-holder float values useful for computing rank correlations\n",
        "    # for better estimates, see `float_to_enqual` above\n",
        "    if enqual == 'FA':\n",
        "        return 6/6\n",
        "    elif enqual == 'GA':\n",
        "        return 5/6\n",
        "    elif enqual == 'B':\n",
        "        return 4/6\n",
        "    elif enqual == 'C':\n",
        "        return 3/6\n",
        "    elif enqual == 'Start':\n",
        "        return 2/6\n",
        "    elif enqual == 'Stub':\n",
        "        return 1/6\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    \n",
        "def stratify_sample(df, sample_size=500, min_per_class=50):\n",
        "    \"\"\"Build stratified sample from quality groundtruth.\n",
        "    \n",
        "    Constraints:\n",
        "    * at least {min_per_class} samples per class (where possible)\n",
        "    * {sample_size} total rows\n",
        "    \"\"\"\n",
        "    df = df.sample(frac=1)  # random order\n",
        "    df['keep'] = False\n",
        "    rows_per_qual = df['enqual'].value_counts().to_dict()\n",
        "    for qc in rows_per_qual:\n",
        "        rows_per_qual[qc] = min(min_per_class, rows_per_qual[qc])\n",
        "    additional_samples = sample_size - sum(rows_per_qual.values())\n",
        "    for i, row in enumerate(df.itertuples()):\n",
        "        if rows_per_qual[row.enqual] > 0:\n",
        "            df.loc[row.Index, 'keep'] = True\n",
        "            rows_per_qual[row.enqual] = rows_per_qual[row.enqual] - 1\n",
        "        elif additional_samples > 0:\n",
        "            df.loc[row.Index, 'keep'] = True\n",
        "            additional_samples -= 1\n",
        "    df = df[df['keep']]\n",
        "    df = df.drop(columns='keep')\n",
        "    return df"
      ],
      "metadata": {
        "id": "7DdYPFyDXdqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_df.to_csv(\"page_assessments.csv\", index=False)"
      ],
      "metadata": {
        "id": "BbAuz6jCXmJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stub_df = en_df.loc[en_df[\"quality\"] == \"Stub\"]\n",
        "stub_df.to_csv(\"stub_assessments.csv\",index=False)"
      ],
      "metadata": {
        "id": "YQ50f-1nXzr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the following code to get the name of the page assosiated with Wikipedia ID and get the average page news"
      ],
      "metadata": {
        "id": "bJIimq11ZhyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Makes the plots appear within the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Two fundamental packages for doing data manipulation\n",
        "import numpy as np                   # http://www.numpy.org/\n",
        "import pandas as pd                  # http://pandas.pydata.org/\n",
        "\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "\n",
        "# Two related packages for plotting data\n",
        "import matplotlib.pyplot as plt      # http://matplotlib.org/\n",
        "import seaborn as sb                 # https://stanford.edu/~mwaskom/software/seaborn/\n",
        "\n",
        "# Package for requesting data via the web and parsing resulting JSON\n",
        "import requests                      # http://docs.python-requests.org/en/master/\n",
        "import json                          # https://docs.python.org/3/library/json.html\n",
        "from bs4 import BeautifulSoup        # https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
        "\n",
        "# Two packages for accessing the MySQL server\n",
        "import pymysql                       # http://pymysql.readthedocs.io/en/latest/\n",
        "import os                            # https://docs.python.org/3.4/library/os.html\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Setup the code environment to use plots with a white background and DataFrames show more columns and rows\n",
        "sb.set_style('whitegrid')\n",
        "pd.options.display.max_columns = 100\n",
        "pd.options.display.max_rows = 110"
      ],
      "metadata": {
        "id": "hlxZdJL0bOQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_title = \"Dog\""
      ],
      "metadata": {
        "id": "LMGbROstbSUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get today's date and yesterday's date\n",
        "today = datetime.today()\n",
        "yesterday = today - timedelta(days = 1)\n",
        "\n",
        "# Convert to strings\n",
        "today_s = datetime.strftime(today,'%Y%m%d00')\n",
        "yesterday_s = datetime.strftime(yesterday,'%Y%m%d00')\n",
        "\n",
        "# Get the pageviews for today and yesterday\n",
        "url_string = 'http://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/{0}/daily/{1}/{2}'\n",
        "print(url_string.format(page_title.replace(' ','_'),yesterday_s,today_s))"
      ],
      "metadata": {
        "id": "XDaldyxZbVVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_daily_pageviews(page_title):\n",
        "    url_string = 'http://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/{0}/daily/2015010100/{1}'\n",
        "    today = datetime.strftime(datetime.today(),'%Y%m%d00')\n",
        "    req = requests.get(url_string.format(page_title,today), headers=headers)\n",
        "    #return req.text\n",
        "    json_s = json.loads(req.text)\n",
        "    if 'items' in json_s.keys():\n",
        "        _df = pd.DataFrame(json_s['items'])[['timestamp','views','article']]\n",
        "        _df['timestamp'] = pd.to_datetime(_df['timestamp'],format='%Y%m%d00')\n",
        "        _df['weekday'] = _df['timestamp'].apply(lambda x:x.weekday())\n",
        "        return int(_df['views'].mean())"
      ],
      "metadata": {
        "id": "DsL92-yycCJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles_df = pd.read_csv(\"stub_titles.csv\")\n",
        "titles_df.head()"
      ],
      "metadata": {
        "id": "XxYpg2NKcIDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles_df['average_views']=titles_df['article_title'].apply(get_daily_pageviews)\n",
        "titles_df.to_csv(\"stub_pageviews.csv\", index=False)"
      ],
      "metadata": {
        "id": "J1tyBsZLc6vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pageview_df = get_daily_pageviews(page_title)\n",
        "print(pageview_df)\n",
        "#pageview_df['timestamp'][2844].day\n",
        "int(pageview_df['views'].mean())\n",
        "# pageview_df.head()"
      ],
      "metadata": {
        "id": "8DF55fnTc7hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def getTitle(id):\n",
        "    # Define the API endpoint URL\n",
        "    url = 'https://en.wikipedia.org/w/api.php'\n",
        "\n",
        "    # Define the parameters for the API call\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'prop': 'info',\n",
        "        'inprop': 'url',\n",
        "        'pageids': id,  # replace with the article ID number\n",
        "        'format': 'json'\n",
        "    }\n",
        "\n",
        "    # Make the API call and parse the response\n",
        "    response = requests.get(url, params=params, headers=headers)\n",
        "    data = json.loads(response.text)\n",
        "\n",
        "    # Get the article title from the response\n",
        "    if 'query' in data and 'pages' in data['query']:\n",
        "        pages = data['query']['pages']\n",
        "        for page_id, page_data in pages.items():\n",
        "            if 'title' in page_data:\n",
        "                title = page_data['title']\n",
        "                return title\n",
        "            else:\n",
        "                return \"N/A\"\n"
      ],
      "metadata": {
        "id": "pBWNMxdec96v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stub_df = pd.read_csv(\"stub_assessments.csv\")"
      ],
      "metadata": {
        "id": "V-8OICDJdGRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stub_df['article_title']=stub_df['page_id'].apply(getTitle)\n",
        "stub_df.to_csv(\"stub_titles.csv\", index=False)"
      ],
      "metadata": {
        "id": "_qO1eBcGdTZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head = stub_df.loc[:4]\n",
        "head"
      ],
      "metadata": {
        "id": "wdw_4ulIdVm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head['article_title']=head['page_id'].apply(getTitle)"
      ],
      "metadata": {
        "id": "pU321yh3dY2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head.head"
      ],
      "metadata": {
        "id": "PHxf7C4RdcvW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}